{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1GGuKkeANSn1KF6dnG-zljwkAVioEPHsI","authorship_tag":"ABX9TyPPdSjHyBKFTpx7p98XpUIQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UbejV_nsSSwY","outputId":"1e226f65-0e85-4e2c-ec20-61c5b6565c21","executionInfo":{"status":"ok","timestamp":1670609078155,"user_tz":300,"elapsed":5875,"user":{"displayName":"Aaron Ogden","userId":"15097074024784650052"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","/content/drive/MyDrive/cs522\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting minisom\n","  Downloading MiniSom-2.3.0.tar.gz (8.8 kB)\n","Building wheels for collected packages: minisom\n","  Building wheel for minisom (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for minisom: filename=MiniSom-2.3.0-py3-none-any.whl size=9016 sha256=a8102b72bb276a4a86e3e1c04c1f2395115f284dcdf36980fe228ff6e18cd6d5\n","  Stored in directory: /root/.cache/pip/wheels/6d/4e/9e/a95c14a232a196c22d9c04b221ff5d25461a1a4c55339c61db\n","Successfully built minisom\n","Installing collected packages: minisom\n","Successfully installed minisom-2.3.0\n"]}],"source":["import os\n","import pandas as pd\n","print(os.getcwd())\n","#from google.colab import drive\n","#MOUNTPOINT = '/content/gdrive'\n","#drive.mount(MOUNTPOINT)\n","#os.chdir('/content/gdrive/MyDrive/cs522')\n","os.chdir('/content/drive/MyDrive/cs522') # drive is permanently mounted now hurray\n","print(os.getcwd())\n","!pip install minisom\n","from minisom import MiniSom"]},{"cell_type":"markdown","source":["Kaggle stopped working so here we are\n","Goal is to create the data to be used in clustering the precipitation data for each survey year + route number combo.\n"],"metadata":{"id":"Ga18U5w4UL1f"}},{"cell_type":"markdown","source":["First we have to associate each route with a file based on closest gps coordinates; and then we need a function that can return a series of data based on coordinates and time period; we'll transform the data in another fn\n","\n","All of the ordinal dates in consolidated.csv are already transformed, so we have to use Runs.csv."],"metadata":{"id":"8amJ87bqVdSe"}},{"cell_type":"code","source":["import numpy\n","from sklearn.preprocessing import MinMaxScaler\n","\n","def distsqrd(x1,y1,x2,y2) : \n","  return ((x2-x1)*(x2-x1)) + ((y2-y1)*(y2-y1)) # return distance squared\n","\n","def returnClosestFile(lat, lng, arraywithfilename) :\n","  winner = 99999999\n","  winningIndex = 0\n","  currentIndex = 0\n","  for x in arraywithfilename :\n","    current = distsqrd(lng,lat,x[1], x[0])\n","    if current < winner :\n","      winner = current\n","      winningIndex = currentIndex\n","    currentIndex += 1\n","  return arraywithfilename[winningIndex][2] # return file name\n","\n","def returnTimeSeriesBasedOnTimeRange(dataframeFromPrecipFile, startDate, endDate) :\n","  tempDataframe = dataframeFromPrecipFile.set_index('DATE')\n","  tempNumpy = tempDataframe[startDate : endDate].PRCP.to_numpy()\n","  tempNumpy = numpy.nan_to_num(tempNumpy)\n","  return tempNumpy\n","\n","# read a precipitation file and return DataFrame\n","def readprecip(filename) :\n","  return pd.read_csv(filename, parse_dates=['DATE'])\n","\n","#testframe = returnTimeSeriesBasedOnTimeRange(readprecip('3163000.csv'), '2001-01-01', '2001-01-15')\n","#print(testframe)\n","\n","# returns a dictionary that matches run id to survey date\n","def readRunsRetrieveDates(filepath) :\n","  runsDF = pd.read_csv(filepath, parse_dates=['SurveyDate']) # I'm glad for the magic that is the date parser\n","  #print(runsDF.head())\n","  #print(runsDF.dtypes)\n","  returnValue = {}\n","  for index, row in runsDF.iterrows() :\n","    returnValue[row.RunID] = row.SurveyDate\n","  return returnValue\n","\n","#runsdict = readRunsRetrieveDates('Runs.csv')\n","#print(runsdict[189])\n","\n","# returns a DataFrame which can use .loc[route number] to retrieve coordinates\n","def readRouteUnitCoordinates(filepath) :\n","  tmp = pd.read_csv(filepath)\n","  return tmp.set_index('RouteNumber')\n","\n","#uc = readRouteUnitCoordinates('RouteUnitCoordinates.csv')\n","#print(uc.loc[250122])\n","\n","def readConsolidated(filepath) :\n","  return pd.read_csv(filepath)\n","\n","coordarray = [[ 44.53845502, -93.83408473, '3163000.csv'],\n","       [ 39.0125231 , -79.95821355, '3163009.csv'],\n","       [ 30.96771759, -84.4666357, '3163013.csv' ],\n","       [ 43.29577165, -71.80487573, '3163016.csv'],\n","       [ 31.72070082, -90.2132503, '3155781-mod.csv' ],\n","       [ 39.22193018, -76.35985149, '3155785.csv'],\n","       [ 39.76749934, -86.2771057, '3155808.csv' ],\n","       [ 35.87956162, -76.99912934, '3155861.csv'],\n","       [ 38.11810374, -92.30672146, '3155871.csv'],\n","       [ 34.73908715, -83.79985733, '3155873.csv'],\n","       [ 45.23027738, -68.96514196, '3155888.csv'],\n","       [ 47.02496426, -95.2990422, '3155894.csv' ],\n","       [ 40.91172204, -74.4773553, '3155906.csv' ],\n","       [ 30.60071905, -94.11526452, '3155996.csv'],\n","       [ 28.08420413, -81.70594462, '3156005.csv'],\n","       [ 33.11577242, -81.17899961, '3156014.csv'],\n","       [ 41.49745322, -93.02686429, '3156020.csv'],\n","       [ 47.33513513, -92.26729451, '3156028.csv'],\n","       [ 35.46922886, -88.55395709,'3156036.csv'],\n","       [ 35.63904872, -80.02362788, '3156042.csv']]\n","coorddict = {}\n","\n","# returns a dictionary mapping file name to DataFrame\n","def readAllPrecipFiles(coray) :\n","  filedict = {}\n","  for x in coray :\n","    # associate file name with the data from that file\n","    filedict[x[2]] = readprecip(x[2])\n","  return filedict\n","\n","def normalizeIV (interpVals) :\n","  llv2 = []\n","  for x in interpVals : # each element has to be turned into a list\n","    llv2.append([x])\n","    t2 = MinMaxScaler().fit_transform(llv2)\n","    t2 = t2.reshape(len(t2))\n","  #print(t2) # looks fine\n","  return t2\n","\n","def outputfn(coray) :\n","  successes = 0\n","  attempts = 0\n","  # return value, dictionary route then year then precip data\n","  routeThenYearOut = {}\n","  # read all precipitation files\n","  precips = readAllPrecipFiles(coray)\n","  # read RouteUnitCoordinates.csv\n","  routeToCoordDF = readRouteUnitCoordinates('RouteUnitCoordinates.csv')\n","  # read Runs.csv\n","  runIDToDateDict = readRunsRetrieveDates('Runs.csv')\n","  # read consolidated.csv\n","  concsv = readConsolidated('consolidated.csv')\n","  #print(concsv.describe().T)\n","  #print(concsv.head())\n","  #print(concsv.tail(10))\n","  # iterate through it\n","  #for index, row in concsv.iterrows() :\n","  for index in range(6118) :\n","    row = concsv.iloc[index]\n","    #print(index)\n","    attempts += 1\n","    if pd.isna(row.RunID1) :\n","      continue\n","    if pd.isna(row.RunID2) :\n","      continue\n","    if pd.isna(row.RunID3) :\n","      continue\n","    # add dict if needed\n","    if row.RouteNumber not in routeThenYearOut :\n","      routeThenYearOut[row.RouteNumber] = {}\n","    # determine the first and last runs of a given year+route\n","    firstrun = runIDToDateDict[row.RunID1]\n","    lastrun = runIDToDateDict[row.RunID3]\n","    if not pd.isna(row.RunID4) :\n","      lastrun = runIDToDateDict[row.RunID4]\n","    # find the coordinates of the route\n","    if row.RouteNumber in routeToCoordDF.index :\n","      coordSeries = routeToCoordDF.loc[row.RouteNumber]\n","    else :\n","      continue\n","    # find the closest weather file, retrieve the previously read-in precip data\n","    precipFileName = returnClosestFile(coordSeries.lat, coordSeries.lon, coray)\n","    precipDF = precips[precipFileName]\n","    # retrieve data between first and last runs - future work would be to retrieve data before the first run as well\n","    periodSeries = returnTimeSeriesBasedOnTimeRange(precipDF, firstrun, lastrun)\n","    if len(periodSeries) == 0 :\n","      #print(precipDF,firstrun,lastrun)\n","      continue\n","    #print(precipFileName, coordSeries.lat, coordSeries.lon, periodSeries) # run a litte sanity check on the result\n","    # apparently numpy was talking about the x coordinates have to be monotonically increasing, not the values, so I can use it\n","    x = []\n","    centi = len(periodSeries) / 100\n","    for i in range(100) :\n","      # we have to fit whatever data range size we have into 100 units, chosen because it's a nice round number and is close to the actual data series lengths in some cases\n","      x.append(i*centi)\n","    xp = []\n","    for i in range(len(periodSeries)) :\n","      xp.append(i)\n","    #if True in numpy.isnan(periodSeries) :\n","    #  print(periodSeries, precipFileName, firstrun, lastrun)  # tracking down missing values\n","    #  break\n","    interpolatedValues = numpy.interp(x,xp,periodSeries)\n","    routeThenYearOut[row.RouteNumber][row.SurveyYear] = normalizeIV(interpolatedValues)\n","    successes += 1\n","    #print(interpolatedValues)\n","    #break\n","  print(attempts,successes)\n","  return routeThenYearOut\n","\n","try :\n","  routeThenYearPrecipatation = outputfn(coordarray)\n","except Exception as e:\n","  print(e)\n","print(len(routeThenYearPrecipatation))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n47n4GE_VZ0H","executionInfo":{"status":"ok","timestamp":1670609247133,"user_tz":300,"elapsed":148369,"user":{"displayName":"Aaron Ogden","userId":"15097074024784650052"}},"outputId":"9a5acf7c-80ce-44b5-8a79-d52ee50ece59"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["6118 4887\n","1120\n"]}]},{"cell_type":"code","source":["# now that we have the time series, we can run MiniSOM on them\n","normalizedScoresOverTime = []\n","routeList = []\n","yearList = []\n","for routeNumber, yearDict in routeThenYearPrecipatation.items() :\n","  for yearKey, scoreList in yearDict.items() :\n","    normalizedScoresOverTime.append(scoreList)\n","    routeList.append(routeNumber)\n","    yearList.append(yearKey)\n","# Trying to get a map such that not every data point is its own centroid\n","map_side_length = 30\n","# default arguments, starting out matching side length to ~square root of number of items, we'll go from there\n","asom = MiniSom(map_side_length, map_side_length, len(normalizedScoresOverTime[0])) \n","asom.random_weights_init(normalizedScoresOverTime)\n","asom.train(normalizedScoresOverTime, 50000)\n","# \n","print(\"training done\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mDUeQYu9bdB5","executionInfo":{"status":"ok","timestamp":1670617480197,"user_tz":300,"elapsed":55837,"user":{"displayName":"Aaron Ogden","userId":"15097074024784650052"}},"outputId":"2c6624e8-8706-4b49-96b6-3788c9832937"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["training done\n"]}]},{"cell_type":"code","source":["# With training complete, we need to store the winning centroids so we can do associative analysis later\n","#for i in range(len(normalizedScoresOverTime)) :\n","  #print(asom.winner(normalizedScoresOverTime[i]))\n","\n","# I'm still seeing a lot of 1s and 2s, so I might need to decrease the map size again\n","for (row,column), value  in asom.win_map(normalizedScoresOverTime).items() :\n","  print(row,column,len(value))"],"metadata":{"id":"DzIjJx1pDyuz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# need to write all this to a file so we can run association analysis\n","xlist = []\n","ylist = []\n","for x in normalizedScoresOverTime :\n","  (xwin, ywin) = asom.winner(x)\n","  xlist.append(xwin)\n","  ylist.append(ywin)\n","  #print(asom.winner(x)) # printed out looks okay\n","  #print(xwin,ywin)\n","  #break\n","outDataFrame = pd.DataFrame({\n","    'RouteNumber': routeList,\n","    'SurveyYear': yearList,\n","    'x': xlist,\n","    'y': ylist})\n","outDataFrame.to_csv('precipWinners.csv')"],"metadata":{"id":"Dd7mRl-xjSmA"},"execution_count":null,"outputs":[]}]}